"""
Model Comparison and Visualization Tool
Visualize and compare performance of multiple ML models

ğŸ“Š MÃ” Táº¢ CÃC CHá»ˆ Sá» ÄÃNH GIÃ MODEL
================================================================================

1. RÂ² SCORE (Coefficient of Determination)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Ã nghÄ©a: Äo lÆ°á»ng kháº£ nÄƒng model giáº£i thÃ­ch biáº¿n thiÃªn cá»§a dá»¯ liá»‡u
   â€¢ Khoáº£ng giÃ¡ trá»‹: -âˆ Ä‘áº¿n 1 (lÃ½ tÆ°á»Ÿng = 1)
   â€¢ Train RÂ²: Hiá»‡u suáº¥t trÃªn táº­p huáº¥n luyá»‡n
   â€¢ Test RÂ²: Hiá»‡u suáº¥t trÃªn táº­p kiá»ƒm tra (quan trá»ng nháº¥t)
   â€¢ CV RÂ² Mean: Trung bÃ¬nh RÂ² qua cross-validation
   â€¢ ÄÃ¡nh giÃ¡:
     - RÂ² > 0.99: â­ EXCELLENT (Xuáº¥t sáº¯c)
     - RÂ² > 0.95: âœ“ VERY GOOD (Ráº¥t tá»‘t)
     - RÂ² > 0.90: â—‹ GOOD (Tá»‘t)
     - RÂ² < 0.90: â–³ NEEDS IMPROVEMENT (Cáº§n cáº£i thiá»‡n)

2. RMSE (Root Mean Squared Error)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Ã nghÄ©a: Sai sá»‘ trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿
   â€¢ ÄÆ¡n vá»‹: CÃ¹ng Ä‘Æ¡n vá»‹ vá»›i biáº¿n má»¥c tiÃªu
   â€¢ Äáº·c Ä‘iá»ƒm: Pháº¡t náº·ng cÃ¡c sai sá»‘ lá»›n hÆ¡n MAE
   â€¢ Má»¥c tiÃªu: CÃ ng nhá» cÃ ng tá»‘t (gáº§n 0)
   â€¢ á»¨ng dá»¥ng: ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ, nháº¡y cáº£m vá»›i outliers

3. MAE (Mean Absolute Error)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Ã nghÄ©a: Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh
   â€¢ ÄÆ¡n vá»‹: CÃ¹ng Ä‘Æ¡n vá»‹ vá»›i biáº¿n má»¥c tiÃªu
   â€¢ Äáº·c Ä‘iá»ƒm: Ãt nháº¡y cáº£m vá»›i outliers hÆ¡n RMSE
   â€¢ Má»¥c tiÃªu: CÃ ng nhá» cÃ ng tá»‘t (gáº§n 0)
   â€¢ á»¨ng dá»¥ng: Äo lÆ°á»ng sai sá»‘ trung bÃ¬nh thá»±c táº¿

4. CV RÂ² Std (Cross-Validation Standard Deviation)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Ã nghÄ©a: Äá»™ á»•n Ä‘á»‹nh cá»§a model qua cÃ¡c fold cross-validation
   â€¢ Má»¥c tiÃªu: CÃ ng nhá» cÃ ng tá»‘t
   â€¢ ÄÃ¡nh giÃ¡:
     - Std < 0.001: Ráº¥t á»•n Ä‘á»‹nh
     - Std < 0.005: á»”n Ä‘á»‹nh
     - Std > 0.005: KhÃ´ng á»•n Ä‘á»‹nh
   â€¢ á»¨ng dá»¥ng: ÄÃ¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

5. OVERFITTING GAP (Train RÂ² - Test RÂ²)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ Ã nghÄ©a: Má»©c Ä‘á»™ overfitting cá»§a model
   â€¢ ÄÃ¡nh giÃ¡:
     - Gap < 0.01: âœ“ KhÃ´ng overfitting
     - Gap 0.01-0.05: âš  Overfitting nháº¹
     - Gap > 0.05: âŒ Overfitting nghiÃªm trá»ng
   â€¢ LÆ°u Ã½: Gap Ã¢m (Test > Train) cÃ³ thá»ƒ do regularization máº¡nh

6. COMPOSITE SCORE (Äiá»ƒm Tá»•ng Há»£p)
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â€¢ CÃ´ng thá»©c: 40% Test RÂ² + 30% RMSEâ»Â¹ + 20% MAEâ»Â¹ + 10% CV Stability
   â€¢ Khoáº£ng giÃ¡ trá»‹: 0 Ä‘áº¿n 1
   â€¢ Má»¥c Ä‘Ã­ch: Xáº¿p háº¡ng tá»•ng thá»ƒ cÃ¡c model dá»±a trÃªn nhiá»u tiÃªu chÃ­
   â€¢ á»¨ng dá»¥ng: Chá»n model tá»‘t nháº¥t cÃ¢n báº±ng giá»¯a accuracy vÃ  stability

ğŸ“ˆ PHÃ‚N TÃCH Káº¾T QUáº¢ SO SÃNH
================================================================================

Dá»±a trÃªn káº¿t quáº£ thá»±c nghiá»‡m vá»›i 7 models:

TOP PERFORMERS:
â€¢ Ridge Regression: Test RÂ² = 0.999986 (gáº§n nhÆ° hoÃ n háº£o)
  - Sai sá»‘ cá»±c tháº¥p (RMSE=0.000427, MAE=0.000342)
  - Ráº¥t á»•n Ä‘á»‹nh (CV Std=0.000003)
  - KhÃ´ng cÃ³ overfitting (Gap=-0.000001)
  â†’ Best choice cho production

â€¢ CatBoost: Test RÂ² = 0.997946
  - Hiá»‡u suáº¥t xuáº¥t sáº¯c vá»›i ensemble boosting
  - Sai sá»‘ tháº¥p (RMSE=0.005185)
  - Ráº¥t á»•n Ä‘á»‹nh (CV Std=0.000552)
  â†’ Alternative tá»‘t cho dá»¯ liá»‡u phá»©c táº¡p

â€¢ LightGBM: Test RÂ² = 0.991299
  - Tá»‘c Ä‘á»™ training nhanh
  - Hiá»‡u suáº¥t cao (RMSE=0.010672)
  - á»”n Ä‘á»‹nh (CV Std=0.001410)
  â†’ Tá»‘t cho datasets lá»›n

MEDIUM PERFORMERS:
â€¢ Gradient Boosting: Test RÂ² = 0.988513
â€¢ Random Forest: Test RÂ² = 0.964002
â€¢ XGBoost: Test RÂ² = 0.951353
  â†’ PhÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n chuáº©n

POOR PERFORMERS:
â€¢ Lasso Regression: Test RÂ² = -0.001632
  - Regularization quÃ¡ máº¡nh
  - KhÃ´ng phÃ¹ há»£p vá»›i dá»¯ liá»‡u nÃ y
  â†’ KhÃ´ng nÃªn sá»­ dá»¥ng

ğŸ¯ KHUYáº¾N NGHá»Š
================================================================================

1. Sá»¬ Dá»¤NG CHO PRODUCTION:
   â†’ Ridge Regression (RÂ²=0.9999, RMSE=0.0004)
   LÃ½ do: Accuracy cao nháº¥t, á»•n Ä‘á»‹nh, khÃ´ng overfitting

2. BACKUP MODEL:
   â†’ CatBoost (RÂ²=0.9979, RMSE=0.0052)
   LÃ½ do: Hiá»‡u suáº¥t tuyá»‡t vá»i, xá»­ lÃ½ tá»‘t categorical features

3. KHI Cáº¦N Tá»C Äá»˜:
   â†’ LightGBM (RÂ²=0.9913, RMSE=0.0107)
   LÃ½ do: Training nhanh, hiá»‡u suáº¥t cao

4. TRÃNH Sá»¬ Dá»¤NG:
   â†’ Lasso Regression
   LÃ½ do: Hiá»‡u suáº¥t kÃ©m vá»›i dataset nÃ y

================================================================================
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
from pathlib import Path
import warnings

warnings.filterwarnings('ignore')

# Set style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

class ModelComparator:
    """Compare and visualize multiple trained models"""
    
    def __init__(self, results_path='model_comparison_results.csv'):
        """
        Initialize ModelComparator
        
        Args:
            results_path: Path to model comparison results CSV
        """
        self.results_path = results_path
        self.results_df = None
        self.output_dir = 'visualization_outputs'
        
        # Create output directory
        os.makedirs(self.output_dir, exist_ok=True)
        
    def load_results(self):
        """Load model comparison results"""
        print(f"Loading results from {self.results_path}...")
        self.results_df = pd.read_csv(self.results_path, index_col=0)
        print(f"Loaded {len(self.results_df)} models")
        print(self.results_df)
        return self.results_df
    
    def plot_r2_comparison(self):
        """Plot RÂ² scores comparison across all models"""
        fig, ax = plt.subplots(figsize=(14, 8))
        
        models = self.results_df.index
        train_r2 = self.results_df['Train RÂ²']
        test_r2 = self.results_df['Test RÂ²']
        cv_r2 = self.results_df['CV RÂ² Mean']
        
        x = np.arange(len(models))
        width = 0.25
        
        bars1 = ax.bar(x - width, train_r2, width, label='Train RÂ²', alpha=0.8)
        bars2 = ax.bar(x, test_r2, width, label='Test RÂ²', alpha=0.8)
        bars3 = ax.bar(x + width, cv_r2, width, label='CV RÂ² Mean', alpha=0.8)
        
        ax.set_xlabel('Models', fontsize=12, fontweight='bold')
        ax.set_ylabel('RÂ² Score', fontsize=12, fontweight='bold')
        ax.set_title('RÂ² Score Comparison Across Models', fontsize=14, fontweight='bold')
        ax.set_xticks(x)
        ax.set_xticklabels(models, rotation=45, ha='right')
        ax.legend(fontsize=10)
        ax.grid(True, alpha=0.3)
        
        # Add value labels on bars
        for bars in [bars1, bars2, bars3]:
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.4f}',
                       ha='center', va='bottom', fontsize=8, rotation=0)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/r2_comparison.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/r2_comparison.png")
        plt.show()
        
    def plot_error_metrics(self):
        """Plot error metrics (RMSE and MAE)"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        models = self.results_df.index
        rmse = self.results_df['Test RMSE']
        mae = self.results_df['Test MAE']
        
        # RMSE plot
        bars1 = ax1.barh(models, rmse, color='coral', alpha=0.8)
        ax1.set_xlabel('RMSE', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Models', fontsize=12, fontweight='bold')
        ax1.set_title('Root Mean Squared Error (RMSE)', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for i, bar in enumerate(bars1):
            width = bar.get_width()
            ax1.text(width, bar.get_y() + bar.get_height()/2.,
                    f'{width:.6f}',
                    ha='left', va='center', fontsize=9)
        
        # MAE plot
        bars2 = ax2.barh(models, mae, color='skyblue', alpha=0.8)
        ax2.set_xlabel('MAE', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Models', fontsize=12, fontweight='bold')
        ax2.set_title('Mean Absolute Error (MAE)', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for i, bar in enumerate(bars2):
            width = bar.get_width()
            ax2.text(width, bar.get_y() + bar.get_height()/2.,
                    f'{width:.6f}',
                    ha='left', va='center', fontsize=9)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/error_metrics.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/error_metrics.png")
        plt.show()
        
    def plot_overfitting_analysis(self):
        """Analyze overfitting by comparing train vs test RÂ²"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        models = self.results_df.index
        train_r2 = self.results_df['Train RÂ²']
        test_r2 = self.results_df['Test RÂ²']
        
        # Calculate overfitting gap
        overfit_gap = train_r2 - test_r2
        
        colors = ['red' if gap > 0.01 else 'green' for gap in overfit_gap]
        
        bars = ax.barh(models, overfit_gap, color=colors, alpha=0.7)
        ax.set_xlabel('Overfitting Gap (Train RÂ² - Test RÂ²)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Models', fontsize=12, fontweight='bold')
        ax.set_title('Overfitting Analysis', fontsize=14, fontweight='bold')
        ax.axvline(x=0, color='black', linestyle='--', linewidth=1)
        ax.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax.text(width, bar.get_y() + bar.get_height()/2.,
                   f'{width:.6f}',
                   ha='left' if width > 0 else 'right', 
                   va='center', fontsize=9)
        
        # Add legend
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='red', alpha=0.7, label='High Overfitting (>0.01)'),
            Patch(facecolor='green', alpha=0.7, label='Low Overfitting (â‰¤0.01)')
        ]
        ax.legend(handles=legend_elements, loc='best')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/overfitting_analysis.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/overfitting_analysis.png")
        plt.show()
        
    def plot_cv_stability(self):
        """Plot cross-validation stability (mean Â± std)"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        models = self.results_df.index
        cv_mean = self.results_df['CV RÂ² Mean']
        cv_std = self.results_df['CV RÂ² Std']
        
        y_pos = np.arange(len(models))
        
        # Plot with error bars
        ax.barh(y_pos, cv_mean, xerr=cv_std, color='mediumpurple', 
                alpha=0.7, capsize=5, error_kw={'linewidth': 2})
        
        ax.set_yticks(y_pos)
        ax.set_yticklabels(models)
        ax.set_xlabel('CV RÂ² Score', fontsize=12, fontweight='bold')
        ax.set_ylabel('Models', fontsize=12, fontweight='bold')
        ax.set_title('Cross-Validation Stability (Mean Â± Std)', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='x')
        
        # Add value labels
        for i, (mean, std) in enumerate(zip(cv_mean, cv_std)):
            ax.text(mean, i, f'{mean:.4f}Â±{std:.6f}',
                   ha='left', va='center', fontsize=8)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/cv_stability.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/cv_stability.png")
        plt.show()
        
    def plot_radar_chart(self):
        """Create radar chart for top 4 models"""
        # Select top 4 models based on Test RÂ²
        top_models = self.results_df.nlargest(4, 'Test RÂ²')
        
        # Normalize metrics to 0-1 scale
        metrics = ['Test RÂ²', 'CV RÂ² Mean', 'Train RÂ²']
        
        # For RMSE and MAE, use inverse (lower is better)
        normalized_data = top_models[metrics].copy()
        
        # Add inverse error metrics
        max_rmse = self.results_df['Test RMSE'].max()
        max_mae = self.results_df['Test MAE'].max()
        normalized_data['Low RMSE'] = 1 - (top_models['Test RMSE'] / max_rmse)
        normalized_data['Low MAE'] = 1 - (top_models['Test MAE'] / max_mae)
        
        categories = ['Test RÂ²', 'CV RÂ² Mean', 'Train RÂ²', 'Low RMSE', 'Low MAE']
        N = len(categories)
        
        angles = [n / float(N) * 2 * np.pi for n in range(N)]
        angles += angles[:1]
        
        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
        
        for idx, (model_name, row) in enumerate(top_models.iterrows()):
            values = normalized_data.loc[model_name].values.flatten().tolist()
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, label=model_name, color=colors[idx])
            ax.fill(angles, values, alpha=0.15, color=colors[idx])
        
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories, size=10)
        ax.set_ylim(0, 1)
        ax.set_title('Top 4 Models Performance Comparison (Radar Chart)', 
                    size=14, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/radar_chart.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/radar_chart.png")
        plt.show()
        
    def plot_heatmap(self):
        """Create heatmap of all metrics"""
        fig, ax = plt.subplots(figsize=(12, 8))
        
        # Normalize all metrics to 0-1 scale for better visualization
        normalized_df = self.results_df.copy()
        
        # Normalize positive metrics (higher is better)
        for col in ['Train RÂ²', 'Test RÂ²', 'CV RÂ² Mean']:
            normalized_df[col] = normalized_df[col]
        
        # Normalize negative metrics (lower is better) - invert
        max_rmse = normalized_df['Test RMSE'].max()
        max_mae = normalized_df['Test MAE'].max()
        max_cv_std = normalized_df['CV RÂ² Std'].max()
        
        normalized_df['Test RMSE (inv)'] = 1 - (normalized_df['Test RMSE'] / max_rmse)
        normalized_df['Test MAE (inv)'] = 1 - (normalized_df['Test MAE'] / max_mae)
        normalized_df['CV Std (inv)'] = 1 - (normalized_df['CV RÂ² Std'] / max_cv_std)
        
        # Select columns for heatmap
        heatmap_cols = ['Train RÂ²', 'Test RÂ²', 'CV RÂ² Mean', 
                       'Test RMSE (inv)', 'Test MAE (inv)', 'CV Std (inv)']
        heatmap_data = normalized_df[heatmap_cols]
        
        sns.heatmap(heatmap_data, annot=True, fmt='.4f', cmap='RdYlGn', 
                   center=0.5, linewidths=1, linecolor='white',
                   cbar_kws={'label': 'Normalized Score'}, ax=ax)
        
        ax.set_title('Model Performance Heatmap (Normalized Metrics)', 
                    fontsize=14, fontweight='bold', pad=20)
        ax.set_xlabel('Metrics', fontsize=12, fontweight='bold')
        ax.set_ylabel('Models', fontsize=12, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/performance_heatmap.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/performance_heatmap.png")
        plt.show()
        
    def plot_ranking(self):
        """Create ranking visualization based on multiple criteria"""
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # Calculate composite score
        # Higher Test RÂ² is better, lower RMSE is better, lower CV Std is better
        scores = self.results_df.copy()
        
        # Normalize and combine metrics
        test_r2_norm = scores['Test RÂ²']
        rmse_norm = 1 - (scores['Test RMSE'] / scores['Test RMSE'].max())
        cv_std_norm = 1 - (scores['CV RÂ² Std'] / scores['CV RÂ² Std'].max())
        mae_norm = 1 - (scores['Test MAE'] / scores['Test MAE'].max())
        
        # Weighted composite score
        scores['Composite Score'] = (
            test_r2_norm * 0.4 +  # 40% weight on test RÂ²
            rmse_norm * 0.3 +      # 30% weight on RMSE
            mae_norm * 0.2 +       # 20% weight on MAE
            cv_std_norm * 0.1      # 10% weight on CV stability
        )
        
        # Sort by composite score
        scores_sorted = scores.sort_values('Composite Score', ascending=True)
        
        colors = plt.cm.viridis(np.linspace(0, 1, len(scores_sorted)))
        
        bars = ax.barh(scores_sorted.index, scores_sorted['Composite Score'], 
                      color=colors, alpha=0.8)
        
        ax.set_xlabel('Composite Score', fontsize=12, fontweight='bold')
        ax.set_ylabel('Models', fontsize=12, fontweight='bold')
        ax.set_title('Model Ranking (Weighted Composite Score)', 
                    fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='x')
        
        # Add value labels and ranking
        for i, (bar, score) in enumerate(zip(bars, scores_sorted['Composite Score'])):
            width = bar.get_width()
            rank = len(scores_sorted) - i
            ax.text(width, bar.get_y() + bar.get_height()/2.,
                   f'  #{rank}: {score:.4f}',
                   ha='left', va='center', fontsize=10, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/model_ranking.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/model_ranking.png")
        plt.show()
        
        return scores_sorted[['Test RÂ²', 'Test RMSE', 'Test MAE', 'CV RÂ² Std', 'Composite Score']]
        
    def plot_performance_summary(self):
        """Create comprehensive performance summary"""
        fig = plt.figure(figsize=(18, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. Test RÂ² comparison
        ax1 = fig.add_subplot(gs[0, :2])
        test_r2_sorted = self.results_df.sort_values('Test RÂ²', ascending=True)
        colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(test_r2_sorted)))
        bars = ax1.barh(test_r2_sorted.index, test_r2_sorted['Test RÂ²'], color=colors)
        ax1.set_xlabel('Test RÂ² Score', fontweight='bold')
        ax1.set_title('Test RÂ² Performance', fontweight='bold', fontsize=12)
        ax1.grid(True, alpha=0.3, axis='x')
        for bar in bars:
            width = bar.get_width()
            ax1.text(width, bar.get_y() + bar.get_height()/2.,
                    f'{width:.4f}', ha='left', va='center', fontsize=8)
        
        # 2. Error comparison
        ax2 = fig.add_subplot(gs[0, 2])
        error_data = self.results_df[['Test RMSE', 'Test MAE']]
        error_data.plot(kind='bar', ax=ax2, alpha=0.7)
        ax2.set_title('Error Metrics', fontweight='bold', fontsize=12)
        ax2.set_ylabel('Error Value', fontweight='bold')
        ax2.set_xticklabels(ax2.get_xticklabels(), rotation=45, ha='right')
        ax2.legend(fontsize=8)
        ax2.grid(True, alpha=0.3)
        
        # 3. Train vs Test RÂ²
        ax3 = fig.add_subplot(gs[1, :])
        x = np.arange(len(self.results_df))
        width = 0.35
        ax3.bar(x - width/2, self.results_df['Train RÂ²'], width, 
               label='Train RÂ²', alpha=0.8)
        ax3.bar(x + width/2, self.results_df['Test RÂ²'], width, 
               label='Test RÂ²', alpha=0.8)
        ax3.set_xlabel('Models', fontweight='bold')
        ax3.set_ylabel('RÂ² Score', fontweight='bold')
        ax3.set_title('Train vs Test RÂ² Comparison', fontweight='bold', fontsize=12)
        ax3.set_xticks(x)
        ax3.set_xticklabels(self.results_df.index, rotation=45, ha='right')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # 4. Statistics table
        ax4 = fig.add_subplot(gs[2, :])
        ax4.axis('tight')
        ax4.axis('off')
        
        stats_data = []
        for model in self.results_df.index:
            row = self.results_df.loc[model]
            stats_data.append([
                model,
                f"{row['Test RÂ²']:.4f}",
                f"{row['Test RMSE']:.6f}",
                f"{row['Test MAE']:.6f}",
                f"{row['CV RÂ² Mean']:.4f}Â±{row['CV RÂ² Std']:.6f}"
            ])
        
        table = ax4.table(cellText=stats_data,
                         colLabels=['Model', 'Test RÂ²', 'RMSE', 'MAE', 'CV RÂ² (MeanÂ±Std)'],
                         cellLoc='center',
                         loc='center',
                         colWidths=[0.25, 0.15, 0.15, 0.15, 0.3])
        
        table.auto_set_font_size(False)
        table.set_fontsize(9)
        table.scale(1, 2)
        
        # Color code the best values
        for i, model in enumerate(self.results_df.index, start=1):
            if model == self.results_df['Test RÂ²'].idxmax():
                table[(i, 1)].set_facecolor('#90EE90')
            if model == self.results_df['Test RMSE'].idxmin():
                table[(i, 2)].set_facecolor('#90EE90')
            if model == self.results_df['Test MAE'].idxmin():
                table[(i, 3)].set_facecolor('#90EE90')
        
        # Header styling
        for i in range(5):
            table[(0, i)].set_facecolor('#4472C4')
            table[(0, i)].set_text_props(weight='bold', color='white')
        
        plt.suptitle('Comprehensive Model Performance Summary', 
                    fontsize=16, fontweight='bold', y=0.995)
        
        plt.savefig(f'{self.output_dir}/performance_summary.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/performance_summary.png")
        plt.show()
        
    def plot_scatter_predictions(self):
        """Create scatter plot comparing actual vs predicted for top models"""
        # This requires loading predictions - we'll create a demo visualization
        fig, ax = plt.subplots(figsize=(10, 10))
        
        # Get top 3 models
        top_3 = self.results_df.nlargest(3, 'Test RÂ²')
        
        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
        
        # Create diagonal line (perfect prediction)
        ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Prediction', alpha=0.5)
        
        # Add model performance zones
        y_range = np.linspace(0, 1, 100)
        for model_name, color in zip(top_3.index, colors):
            r2 = top_3.loc[model_name, 'Test RÂ²']
            # Simulate prediction scatter based on RÂ²
            noise = np.random.normal(0, np.sqrt(1-r2) * 0.1, 100)
            x = np.linspace(0, 1, 100)
            y = x + noise
            ax.scatter(x, y, alpha=0.5, s=30, label=f'{model_name} (RÂ²={r2:.4f})', color=color)
        
        ax.set_xlabel('Actual Values', fontsize=12, fontweight='bold')
        ax.set_ylabel('Predicted Values', fontsize=12, fontweight='bold')
        ax.set_title('Prediction Accuracy Comparison (Simulated)', fontsize=14, fontweight='bold')
        ax.legend(loc='upper left', fontsize=10)
        ax.grid(True, alpha=0.3)
        ax.set_xlim(0, 1)
        ax.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/scatter_predictions.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/scatter_predictions.png")
        plt.show()
    
    def plot_training_efficiency(self):
        """Analyze model complexity and training efficiency"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        models = self.results_df.index
        test_r2 = self.results_df['Test RÂ²']
        cv_std = self.results_df['CV RÂ² Std']
        
        # Model complexity proxy (based on overfitting)
        complexity = self.results_df['Train RÂ²'] - self.results_df['Test RÂ²']
        
        # Plot 1: Performance vs Complexity
        colors = plt.cm.viridis(np.linspace(0, 1, len(models)))
        scatter1 = ax1.scatter(complexity, test_r2, c=range(len(models)), 
                              cmap='viridis', s=200, alpha=0.7, edgecolors='black', linewidth=2)
        
        for i, model in enumerate(models):
            ax1.annotate(model, (complexity[i], test_r2[i]), 
                        fontsize=9, ha='center', va='bottom')
        
        ax1.set_xlabel('Model Complexity (Overfit Gap)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Test RÂ² Score', fontsize=12, fontweight='bold')
        ax1.set_title('Performance vs Complexity Trade-off', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Stability vs Performance
        scatter2 = ax2.scatter(cv_std, test_r2, c=range(len(models)), 
                              cmap='plasma', s=200, alpha=0.7, edgecolors='black', linewidth=2)
        
        for i, model in enumerate(models):
            ax2.annotate(model, (cv_std[i], test_r2[i]), 
                        fontsize=9, ha='center', va='bottom')
        
        ax2.set_xlabel('CV Standard Deviation (Instability)', fontsize=12, fontweight='bold')
        ax2.set_ylabel('Test RÂ² Score', fontsize=12, fontweight='bold')
        ax2.set_title('Performance vs Stability Trade-off', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/training_efficiency.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/training_efficiency.png")
        plt.show()
    
    def plot_metrics_distribution(self):
        """Create box plot of metric distributions"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        metrics = ['Train RÂ²', 'Test RÂ²', 'Test RMSE', 'Test MAE']
        
        for ax, metric in zip(axes.flat, metrics):
            data = self.results_df[metric]
            
            # Create violin plot with box plot overlay
            parts = ax.violinplot([data], positions=[0], widths=0.7, 
                                 showmeans=True, showmedians=True)
            
            # Customize violin plot colors
            for pc in parts['bodies']:
                pc.set_facecolor('#8B9DC3')
                pc.set_alpha(0.7)
            
            # Add individual points
            y = data.values
            x = np.random.normal(0, 0.04, size=len(y))
            colors = plt.cm.Set3(np.linspace(0, 1, len(y)))
            
            for i, (xi, yi, model) in enumerate(zip(x, y, data.index)):
                ax.scatter(xi, yi, s=150, alpha=0.8, color=colors[i], 
                          edgecolors='black', linewidth=1.5, zorder=3)
                ax.text(xi + 0.15, yi, model, fontsize=8, va='center')
            
            ax.set_ylabel(metric, fontsize=12, fontweight='bold')
            ax.set_title(f'Distribution of {metric}', fontsize=12, fontweight='bold')
            ax.set_xticks([])
            ax.grid(True, alpha=0.3, axis='y')
            
            # Add statistics
            mean_val = data.mean()
            median_val = data.median()
            std_val = data.std()
            
            stats_text = f'Mean: {mean_val:.6f}\nMedian: {median_val:.6f}\nStd: {std_val:.6f}'
            ax.text(0.5, 0.05, stats_text, transform=ax.transAxes, 
                   fontsize=9, verticalalignment='bottom',
                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        plt.suptitle('Metrics Distribution Across All Models', 
                    fontsize=16, fontweight='bold', y=0.995)
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/metrics_distribution.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/metrics_distribution.png")
        plt.show()
    
    def plot_pareto_chart(self):
        """Create Pareto chart showing cumulative performance"""
        fig, ax1 = plt.subplots(figsize=(14, 8))
        
        # Sort by Test RÂ²
        sorted_df = self.results_df.sort_values('Test RÂ²', ascending=False)
        models = sorted_df.index
        test_r2 = sorted_df['Test RÂ²']
        
        # Calculate cumulative percentage
        cumulative = np.cumsum(test_r2) / np.sum(test_r2) * 100
        
        # Bar chart
        x_pos = np.arange(len(models))
        colors = plt.cm.RdYlGn(np.linspace(0.3, 0.9, len(models)))
        bars = ax1.bar(x_pos, test_r2, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
        
        ax1.set_xlabel('Models', fontsize=12, fontweight='bold')
        ax1.set_ylabel('Test RÂ² Score', fontsize=12, fontweight='bold')
        ax1.set_xticks(x_pos)
        ax1.set_xticklabels(models, rotation=45, ha='right')
        ax1.grid(True, alpha=0.3, axis='y')
        
        # Add value labels on bars
        for i, (bar, val) in enumerate(zip(bars, test_r2)):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height,
                    f'{val:.4f}',
                    ha='center', va='bottom', fontsize=9, fontweight='bold')
        
        # Cumulative line
        ax2 = ax1.twinx()
        line = ax2.plot(x_pos, cumulative, 'ro-', linewidth=3, markersize=10, 
                       label='Cumulative %', color='darkred')
        ax2.set_ylabel('Cumulative Percentage (%)', fontsize=12, fontweight='bold', color='darkred')
        ax2.tick_params(axis='y', labelcolor='darkred')
        ax2.set_ylim(0, 110)
        
        # Add percentage labels on line
        for i, (x, y) in enumerate(zip(x_pos, cumulative)):
            ax2.text(x, y + 3, f'{y:.1f}%', ha='center', fontsize=9, 
                    color='darkred', fontweight='bold')
        
        # Add 80% line
        ax2.axhline(y=80, color='blue', linestyle='--', linewidth=2, alpha=0.5, label='80% Line')
        
        ax1.set_title('Pareto Chart - Model Performance Contribution', 
                     fontsize=14, fontweight='bold', pad=20)
        
        # Combined legend
        lines1, labels1 = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax2.legend(lines2, labels2, loc='center right', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/pareto_chart.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/pareto_chart.png")
        plt.show()
    
    def plot_performance_quadrant(self):
        """Create quadrant analysis of performance vs stability"""
        fig, ax = plt.subplots(figsize=(12, 10))
        
        # Metrics for quadrant analysis
        x_metric = self.results_df['Test RÂ²']
        y_metric = 1 / (self.results_df['Test RMSE'] + 1e-10)  # Inverse RMSE (higher is better)
        
        # Calculate medians for quadrant lines
        x_median = x_metric.median()
        y_median = y_metric.median()
        
        # Plot quadrant lines
        ax.axvline(x=x_median, color='gray', linestyle='--', linewidth=2, alpha=0.5)
        ax.axhline(y=y_median, color='gray', linestyle='--', linewidth=2, alpha=0.5)
        
        # Color code by CV stability
        cv_std = self.results_df['CV RÂ² Std']
        colors = plt.cm.RdYlGn_r(cv_std / cv_std.max())
        
        # Scatter plot
        scatter = ax.scatter(x_metric, y_metric, s=300, c=cv_std, 
                           cmap='RdYlGn_r', alpha=0.7, edgecolors='black', linewidth=2)
        
        # Add model labels
        for i, model in enumerate(self.results_df.index):
            ax.annotate(model, (x_metric[i], y_metric[i]), 
                       fontsize=10, ha='center', va='center', fontweight='bold')
        
        # Add colorbar
        cbar = plt.colorbar(scatter, ax=ax)
        cbar.set_label('CV Standard Deviation\n(Lower is Better)', 
                      fontsize=11, fontweight='bold')
        
        # Quadrant labels
        ax.text(0.02, 0.98, 'High Accuracy\nLow Precision', 
               transform=ax.transAxes, fontsize=11, va='top', ha='left',
               bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))
        
        ax.text(0.98, 0.98, 'High Accuracy\nHigh Precision\nâ­ BEST', 
               transform=ax.transAxes, fontsize=11, va='top', ha='right',
               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))
        
        ax.text(0.02, 0.02, 'Low Accuracy\nLow Precision\nâŒ WORST', 
               transform=ax.transAxes, fontsize=11, va='bottom', ha='left',
               bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))
        
        ax.text(0.98, 0.02, 'Low Accuracy\nHigh Precision', 
               transform=ax.transAxes, fontsize=11, va='bottom', ha='right',
               bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.3))
        
        ax.set_xlabel('Test RÂ² Score (Higher is Better)', fontsize=12, fontweight='bold')
        ax.set_ylabel('Inverse RMSE (Higher is Better)', fontsize=12, fontweight='bold')
        ax.set_title('Performance Quadrant Analysis', fontsize=14, fontweight='bold', pad=20)
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'{self.output_dir}/performance_quadrant.png', dpi=300, bbox_inches='tight')
        print(f"Saved: {self.output_dir}/performance_quadrant.png")
        plt.show()
    
    def generate_comparison_report(self):
        """Generate detailed comparison report"""
        report_path = f'{self.output_dir}/model_comparison_report.txt'
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("MODEL COMPARISON REPORT\n")
            f.write(f"Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*80 + "\n\n")
            
            # Overall Statistics
            f.write("OVERALL STATISTICS\n")
            f.write("-"*80 + "\n")
            f.write(f"Total Models Evaluated: {len(self.results_df)}\n")
            f.write(f"Best Test RÂ²: {self.results_df['Test RÂ²'].max():.6f}\n")
            f.write(f"Worst Test RÂ²: {self.results_df['Test RÂ²'].min():.6f}\n")
            f.write(f"Average Test RÂ²: {self.results_df['Test RÂ²'].mean():.6f}\n")
            f.write(f"Std Dev Test RÂ²: {self.results_df['Test RÂ²'].std():.6f}\n\n")
            
            # Individual Model Details
            f.write("DETAILED MODEL PERFORMANCE\n")
            f.write("-"*80 + "\n\n")
            
            for idx, model in enumerate(self.results_df.index, 1):
                row = self.results_df.loc[model]
                f.write(f"{idx}. {model}\n")
                f.write(f"   {'â”€'*70}\n")
                f.write(f"   Train RÂ²:        {row['Train RÂ²']:.6f}\n")
                f.write(f"   Test RÂ²:         {row['Test RÂ²']:.6f}\n")
                f.write(f"   CV RÂ² Mean:      {row['CV RÂ² Mean']:.6f} Â± {row['CV RÂ² Std']:.6f}\n")
                f.write(f"   Test RMSE:       {row['Test RMSE']:.6f}\n")
                f.write(f"   Test MAE:        {row['Test MAE']:.6f}\n")
                f.write(f"   Overfit Gap:     {row['Train RÂ²'] - row['Test RÂ²']:.6f}\n")
                
                # Performance assessment
                if row['Test RÂ²'] > 0.99:
                    perf = "â­ EXCELLENT"
                elif row['Test RÂ²'] > 0.95:
                    perf = "âœ“ VERY GOOD"
                elif row['Test RÂ²'] > 0.90:
                    perf = "â—‹ GOOD"
                else:
                    perf = "â–³ NEEDS IMPROVEMENT"
                
                f.write(f"   Assessment:      {perf}\n\n")
            
            # Ranking
            f.write("MODELS RANKED BY TEST RÂ²\n")
            f.write("-"*80 + "\n")
            sorted_models = self.results_df.sort_values('Test RÂ²', ascending=False)
            for rank, (model, row) in enumerate(sorted_models.iterrows(), 1):
                f.write(f"{rank}. {model:20s} - RÂ²: {row['Test RÂ²']:.6f}, "
                       f"RMSE: {row['Test RMSE']:.6f}\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write("END OF REPORT\n")
            f.write("="*80 + "\n")
        
        print(f"Saved: {report_path}")
        
        # Print summary to console
        print("\n" + "="*80)
        print("ğŸ“„ COMPARISON REPORT SUMMARY")
        print("="*80)
        with open(report_path, 'r', encoding='utf-8') as f:
            print(f.read())
    
    def print_metrics_description(self):
        """Print detailed description of metrics and analysis"""
        print("\n" + "="*80)
        print("ğŸ“Š MÃ” Táº¢ CÃC CHá»ˆ Sá» ÄÃNH GIÃ MODEL")
        print("="*80 + "\n")
        
        print("1. RÂ² SCORE (Coefficient of Determination)")
        print("   " + "â”€"*70)
        print("   â€¢ Ã nghÄ©a: Äo lÆ°á»ng kháº£ nÄƒng model giáº£i thÃ­ch biáº¿n thiÃªn cá»§a dá»¯ liá»‡u")
        print("   â€¢ Khoáº£ng giÃ¡ trá»‹: -âˆ Ä‘áº¿n 1 (lÃ½ tÆ°á»Ÿng = 1)")
        print("   â€¢ Train RÂ²: Hiá»‡u suáº¥t trÃªn táº­p huáº¥n luyá»‡n")
        print("   â€¢ Test RÂ²: Hiá»‡u suáº¥t trÃªn táº­p kiá»ƒm tra (quan trá»ng nháº¥t)")
        print("   â€¢ CV RÂ² Mean: Trung bÃ¬nh RÂ² qua cross-validation")
        print("   â€¢ ÄÃ¡nh giÃ¡:")
        print("     - RÂ² > 0.99: â­ EXCELLENT (Xuáº¥t sáº¯c)")
        print("     - RÂ² > 0.95: âœ“ VERY GOOD (Ráº¥t tá»‘t)")
        print("     - RÂ² > 0.90: â—‹ GOOD (Tá»‘t)")
        print("     - RÂ² < 0.90: â–³ NEEDS IMPROVEMENT (Cáº§n cáº£i thiá»‡n)")
        print()
        
        print("2. RMSE (Root Mean Squared Error)")
        print("   " + "â”€"*70)
        print("   â€¢ Ã nghÄ©a: Sai sá»‘ trung bÃ¬nh bÃ¬nh phÆ°Æ¡ng giá»¯a dá»± Ä‘oÃ¡n vÃ  thá»±c táº¿")
        print("   â€¢ ÄÆ¡n vá»‹: CÃ¹ng Ä‘Æ¡n vá»‹ vá»›i biáº¿n má»¥c tiÃªu")
        print("   â€¢ Äáº·c Ä‘iá»ƒm: Pháº¡t náº·ng cÃ¡c sai sá»‘ lá»›n hÆ¡n MAE")
        print("   â€¢ Má»¥c tiÃªu: CÃ ng nhá» cÃ ng tá»‘t (gáº§n 0)")
        print("   â€¢ á»¨ng dá»¥ng: ÄÃ¡nh giÃ¡ Ä‘á»™ chÃ­nh xÃ¡c tá»•ng thá»ƒ, nháº¡y cáº£m vá»›i outliers")
        print()
        
        print("3. MAE (Mean Absolute Error)")
        print("   " + "â”€"*70)
        print("   â€¢ Ã nghÄ©a: Sai sá»‘ tuyá»‡t Ä‘á»‘i trung bÃ¬nh")
        print("   â€¢ ÄÆ¡n vá»‹: CÃ¹ng Ä‘Æ¡n vá»‹ vá»›i biáº¿n má»¥c tiÃªu")
        print("   â€¢ Äáº·c Ä‘iá»ƒm: Ãt nháº¡y cáº£m vá»›i outliers hÆ¡n RMSE")
        print("   â€¢ Má»¥c tiÃªu: CÃ ng nhá» cÃ ng tá»‘t (gáº§n 0)")
        print("   â€¢ á»¨ng dá»¥ng: Äo lÆ°á»ng sai sá»‘ trung bÃ¬nh thá»±c táº¿")
        print()
        
        print("4. CV RÂ² Std (Cross-Validation Standard Deviation)")
        print("   " + "â”€"*70)
        print("   â€¢ Ã nghÄ©a: Äá»™ á»•n Ä‘á»‹nh cá»§a model qua cÃ¡c fold cross-validation")
        print("   â€¢ Má»¥c tiÃªu: CÃ ng nhá» cÃ ng tá»‘t")
        print("   â€¢ ÄÃ¡nh giÃ¡:")
        print("     - Std < 0.001: Ráº¥t á»•n Ä‘á»‹nh")
        print("     - Std < 0.005: á»”n Ä‘á»‹nh")
        print("     - Std > 0.005: KhÃ´ng á»•n Ä‘á»‹nh")
        print("   â€¢ á»¨ng dá»¥ng: ÄÃ¡nh giÃ¡ kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a")
        print()
        
        print("5. OVERFITTING GAP (Train RÂ² - Test RÂ²)")
        print("   " + "â”€"*70)
        print("   â€¢ Ã nghÄ©a: Má»©c Ä‘á»™ overfitting cá»§a model")
        print("   â€¢ ÄÃ¡nh giÃ¡:")
        print("     - Gap < 0.01: âœ“ KhÃ´ng overfitting")
        print("     - Gap 0.01-0.05: âš  Overfitting nháº¹")
        print("     - Gap > 0.05: âŒ Overfitting nghiÃªm trá»ng")
        print("   â€¢ LÆ°u Ã½: Gap Ã¢m (Test > Train) cÃ³ thá»ƒ do regularization máº¡nh")
        print()
        
        print("6. COMPOSITE SCORE (Äiá»ƒm Tá»•ng Há»£p)")
        print("   " + "â”€"*70)
        print("   â€¢ CÃ´ng thá»©c: 40% Test RÂ² + 30% RMSEâ»Â¹ + 20% MAEâ»Â¹ + 10% CV Stability")
        print("   â€¢ Khoáº£ng giÃ¡ trá»‹: 0 Ä‘áº¿n 1")
        print("   â€¢ Má»¥c Ä‘Ã­ch: Xáº¿p háº¡ng tá»•ng thá»ƒ cÃ¡c model dá»±a trÃªn nhiá»u tiÃªu chÃ­")
        print("   â€¢ á»¨ng dá»¥ng: Chá»n model tá»‘t nháº¥t cÃ¢n báº±ng accuracy vÃ  stability")
        print()
        print("="*80 + "\n")
    
    def print_analysis_summary(self):
        """Print analysis and recommendations based on results"""
        print("\n" + "="*80)
        print("ğŸ“ˆ PHÃ‚N TÃCH Káº¾T QUáº¢ SO SÃNH")
        print("="*80 + "\n")
        
        # Get sorted models
        sorted_by_r2 = self.results_df.sort_values('Test RÂ²', ascending=False)
        
        print("TOP PERFORMERS:")
        print("â”€"*80)
        for i, (model, row) in enumerate(sorted_by_r2.head(3).iterrows()):
            print(f"\nâ€¢ {model}: Test RÂ² = {row['Test RÂ²']:.6f}")
            
            # Characteristics
            if row['Test RMSE'] < 0.01:
                print(f"  - Sai sá»‘ cá»±c tháº¥p (RMSE={row['Test RMSE']:.6f}, MAE={row['Test MAE']:.6f})")
            else:
                print(f"  - Sai sá»‘ tháº¥p (RMSE={row['Test RMSE']:.6f}, MAE={row['Test MAE']:.6f})")
            
            if row['CV RÂ² Std'] < 0.001:
                print(f"  - Ráº¥t á»•n Ä‘á»‹nh (CV Std={row['CV RÂ² Std']:.6f})")
            elif row['CV RÂ² Std'] < 0.005:
                print(f"  - á»”n Ä‘á»‹nh (CV Std={row['CV RÂ² Std']:.6f})")
            else:
                print(f"  - Äá»™ á»•n Ä‘á»‹nh trung bÃ¬nh (CV Std={row['CV RÂ² Std']:.6f})")
            
            overfit_gap = row['Train RÂ²'] - row['Test RÂ²']
            if abs(overfit_gap) < 0.01:
                print(f"  - KhÃ´ng cÃ³ overfitting (Gap={overfit_gap:.6f})")
            elif overfit_gap < 0.05:
                print(f"  - Overfitting nháº¹ (Gap={overfit_gap:.6f})")
            else:
                print(f"  - CÃ³ dáº¥u hiá»‡u overfitting (Gap={overfit_gap:.6f})")
            
            # Recommendations
            if i == 0:
                print("  â†’ Best choice cho production")
            elif i == 1:
                print("  â†’ Alternative tá»‘t, cÃ³ thá»ƒ lÃ m backup model")
            else:
                print("  â†’ PhÃ¹ há»£p cho cÃ¡c trÆ°á»ng há»£p Ä‘áº·c biá»‡t")
        
        # Medium performers
        if len(sorted_by_r2) > 3:
            print("\n\nMEDIUM PERFORMERS:")
            print("â”€"*80)
            medium_models = sorted_by_r2.iloc[3:6]
            for model, row in medium_models.iterrows():
                print(f"â€¢ {model}: Test RÂ² = {row['Test RÂ²']:.6f}")
            print("  â†’ PhÃ¹ há»£p cho cÃ¡c bÃ i toÃ¡n chuáº©n, cÃ³ thá»ƒ cáº§n tuning thÃªm")
        
        # Poor performers
        if len(sorted_by_r2) > 6:
            print("\n\nPOOR PERFORMERS:")
            print("â”€"*80)
            poor_models = sorted_by_r2.iloc[6:]
            for model, row in poor_models.iterrows():
                print(f"â€¢ {model}: Test RÂ² = {row['Test RÂ²']:.6f}")
            print("  â†’ KhÃ´ng nÃªn sá»­ dá»¥ng cho dataset nÃ y")
        
        print("\n" + "="*80 + "\n")
        
        # Recommendations
        print("\n" + "="*80)
        print("ğŸ¯ KHUYáº¾N NGHá»Š Sá»¬ Dá»¤NG")
        print("="*80 + "\n")
        
        best_model = sorted_by_r2.index[0]
        best_r2 = sorted_by_r2.iloc[0]['Test RÂ²']
        best_rmse = sorted_by_r2.iloc[0]['Test RMSE']
        
        print(f"1. Sá»¬ Dá»¤NG CHO PRODUCTION:")
        print(f"   â†’ {best_model}")
        print(f"   RÂ² = {best_r2:.4f}, RMSE = {best_rmse:.6f}")
        print(f"   LÃ½ do: Accuracy cao nháº¥t, phÃ¹ há»£p cho mÃ´i trÆ°á»ng production")
        print()
        
        if len(sorted_by_r2) > 1:
            backup_model = sorted_by_r2.index[1]
            backup_r2 = sorted_by_r2.iloc[1]['Test RÂ²']
            backup_rmse = sorted_by_r2.iloc[1]['Test RMSE']
            
            print(f"2. BACKUP MODEL:")
            print(f"   â†’ {backup_model}")
            print(f"   RÂ² = {backup_r2:.4f}, RMSE = {backup_rmse:.6f}")
            print(f"   LÃ½ do: Hiá»‡u suáº¥t tuyá»‡t vá»i, cÃ³ thá»ƒ thay tháº¿ khi cáº§n")
            print()
        
        if len(sorted_by_r2) > 2:
            third_model = sorted_by_r2.index[2]
            third_r2 = sorted_by_r2.iloc[2]['Test RÂ²']
            
            print(f"3. ALTERNATIVE OPTION:")
            print(f"   â†’ {third_model}")
            print(f"   RÂ² = {third_r2:.4f}")
            print(f"   LÃ½ do: CÃ¢n báº±ng giá»¯a performance vÃ  cÃ¡c yáº¿u tá»‘ khÃ¡c")
            print()
        
        # Models to avoid
        worst_models = sorted_by_r2[sorted_by_r2['Test RÂ²'] < 0.9]
        if len(worst_models) > 0:
            print(f"4. TRÃNH Sá»¬ Dá»¤NG:")
            for model in worst_models.index:
                print(f"   â†’ {model}")
            print(f"   LÃ½ do: Hiá»‡u suáº¥t khÃ´ng Ä‘áº¡t yÃªu cáº§u vá»›i dataset nÃ y")
            print()
        
        print("="*80 + "\n")
    
    def generate_all_visualizations(self):
        """Generate all visualizations"""
        print("\n" + "="*60)
        print("GENERATING ALL VISUALIZATIONS")
        print("="*60 + "\n")
        
        self.load_results()
        
        # Print metrics description first
        self.print_metrics_description()
        
        print("\n[1/13] Generating RÂ² comparison...")
        self.plot_r2_comparison()
        
        print("\n[2/13] Generating error metrics...")
        self.plot_error_metrics()
        
        print("\n[3/13] Generating overfitting analysis...")
        self.plot_overfitting_analysis()
        
        print("\n[4/13] Generating CV stability plot...")
        self.plot_cv_stability()
        
        print("\n[5/13] Generating radar chart...")
        self.plot_radar_chart()
        
        print("\n[6/13] Generating performance heatmap...")
        self.plot_heatmap()
        
        print("\n[7/13] Generating model ranking...")
        ranking = self.plot_ranking()
        
        print("\n[8/13] Generating performance summary...")
        self.plot_performance_summary()
        
        print("\n[9/13] Generating scatter predictions...")
        self.plot_scatter_predictions()
        
        print("\n[10/13] Generating training efficiency analysis...")
        self.plot_training_efficiency()
        
        print("\n[11/13] Generating metrics distribution...")
        self.plot_metrics_distribution()
        
        print("\n[12/13] Generating Pareto chart...")
        self.plot_pareto_chart()
        
        print("\n[13/13] Generating performance quadrant...")
        self.plot_performance_quadrant()
        
        print("\n[BONUS] Generating comparison report...")
        self.generate_comparison_report()
        
        print("\n" + "="*60)
        print("ALL VISUALIZATIONS COMPLETED!")
        print(f"Output directory: {self.output_dir}/")
        print("="*60 + "\n")
        
        print("\nğŸ“Š MODEL RANKING:")
        print("="*60)
        print(ranking.to_string())
        print("="*60 + "\n")
        
        # Best model summary
        best_model = self.results_df['Test RÂ²'].idxmax()
        best_r2 = self.results_df.loc[best_model, 'Test RÂ²']
        best_rmse = self.results_df.loc[best_model, 'Test RMSE']
        best_mae = self.results_df.loc[best_model, 'Test MAE']
        
        print("\nğŸ† BEST MODEL:")
        print(f"   Model: {best_model}")
        print(f"   Test RÂ²: {best_r2:.6f}")
        print(f"   Test RMSE: {best_rmse:.6f}")
        print(f"   Test MAE: {best_mae:.6f}")
        print("="*60 + "\n")
        
        # Print detailed analysis
        self.print_analysis_summary()


def main():
    """Main execution function"""
    # Initialize comparator
    comparator = ModelComparator('model_comparison_results.csv')
    
    # Generate all visualizations
    comparator.generate_all_visualizations()
    
    print("\nâœ… All visualizations have been saved successfully!")
    print(f"ğŸ“ Check the '{comparator.output_dir}' folder for all charts.\n")


if __name__ == "__main__":
    main()

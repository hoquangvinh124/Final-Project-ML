"""
Import ML Prediction Results to Database
Loads data from CSV files generated by Prophet models into MySQL
"""
import pandas as pd
import mysql.connector
from pathlib import Path
import sys
from datetime import datetime
import json

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.database import DatabaseManager
from utils.config import DB_CONFIG


class PredictionImporter:
    """Import prediction data from CSV to database"""

    def __init__(self):
        self.db = DatabaseManager()
        self.base_dir = Path(__file__).parent.parent / 'revenue_forecasting'
        self.results_dir = self.base_dir / 'results'
        self.data_dir = self.base_dir / 'data'

    def import_store_metadata(self):
        """Import store metadata from CSV"""
        print("=" * 60)
        print("IMPORTING STORE METADATA")
        print("=" * 60)

        metadata_file = self.base_dir / 'ml-models' / 'store_models' / 'stores_metadata.csv'

        if not metadata_file.exists():
            print(f"‚ö†Ô∏è  Metadata file not found: {metadata_file}")
            return False

        try:
            # Read CSV
            df = pd.read_csv(metadata_file)
            print(f"ƒê·ªçc {len(df)} stores t·ª´ CSV")

            # Clear existing data
            self.db.execute_query("DELETE FROM store_predictions")
            self.db.execute_query("DELETE FROM store_metadata")

            # Prepare data
            insert_query = """
                INSERT INTO store_metadata
                (store_nbr, city, state, type, cluster, total_revenue, avg_daily_sales, std_sales, total_transactions)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                ON DUPLICATE KEY UPDATE
                    city = VALUES(city),
                    state = VALUES(state),
                    type = VALUES(type),
                    cluster = VALUES(cluster),
                    total_revenue = VALUES(total_revenue),
                    avg_daily_sales = VALUES(avg_daily_sales),
                    std_sales = VALUES(std_sales),
                    total_transactions = VALUES(total_transactions)
            """

            data = []
            for _, row in df.iterrows():
                # Map CSV columns to database columns
                # CSV has: historical_total_revenue, historical_avg_daily, historical_std_daily, data_points
                # DB needs: total_revenue, avg_daily_sales, std_sales, total_transactions

                total_revenue = float(row.get('historical_total_revenue', row.get('total_revenue', 0)))
                avg_daily_sales = float(row.get('historical_avg_daily', row.get('avg_daily_sales', 0)))
                std_sales = float(row.get('historical_std_daily', row.get('std_sales', 0)))
                total_transactions = int(row.get('data_points', row.get('total_transactions', 0)))

                data.append((
                    int(row['store_nbr']),
                    str(row['city']),
                    str(row['state']),
                    str(row['type']),
                    int(row['cluster']),
                    total_revenue,
                    avg_daily_sales,
                    std_sales,
                    total_transactions
                ))

            # Insert data
            self.db.execute_many(insert_query, data)
            print(f"‚úì Imported {len(data)} stores")

            return True

        except Exception as e:
            print(f"‚úó Error importing store metadata: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def import_overall_predictions(self):
        """Import overall system predictions"""
        print("\n" + "=" * 60)
        print("IMPORTING OVERALL PREDICTIONS")
        print("=" * 60)

        forecast_file = self.results_dir / 'prophet_forecast_full.csv'

        if not forecast_file.exists():
            print(f"‚ö†Ô∏è  Forecast file not found: {forecast_file}")
            return False

        try:
            # Read CSV
            df = pd.read_csv(forecast_file)
            print(f"ƒê·ªçc {len(df)} predictions t·ª´ CSV")

            # Clear existing data
            self.db.execute_query("DELETE FROM overall_predictions")

            # Prepare data
            insert_query = """
                INSERT INTO overall_predictions
                (ds, yhat, yhat_lower, yhat_upper, is_historical)
                VALUES (%s, %s, %s, %s, %s)
            """

            # Determine historical vs future
            today = datetime.now().date()

            data = []
            for _, row in df.iterrows():
                forecast_date = pd.to_datetime(row['ds']).date()
                is_historical = forecast_date < today

                data.append((
                    forecast_date,
                    float(row['yhat']),
                    float(row['yhat_lower']) if 'yhat_lower' in row else None,
                    float(row['yhat_upper']) if 'yhat_upper' in row else None,
                    is_historical
                ))

            # Insert in batches
            batch_size = 1000
            total_inserted = 0

            for i in range(0, len(data), batch_size):
                batch = data[i:i + batch_size]
                self.db.execute_many(insert_query, batch)
                total_inserted += len(batch)
                print(f"  Inserted {total_inserted}/{len(data)} rows...", end='\r')

            print(f"\n‚úì Imported {len(data)} overall predictions")

            # Stats
            stats = self.db.fetch_one("""
                SELECT
                    COUNT(*) as total,
                    SUM(CASE WHEN is_historical THEN 1 ELSE 0 END) as historical,
                    SUM(CASE WHEN NOT is_historical THEN 1 ELSE 0 END) as future,
                    MIN(ds) as first_date,
                    MAX(ds) as last_date
                FROM overall_predictions
            """)

            print(f"  Total: {stats['total']}")
            print(f"  Historical: {stats['historical']}")
            print(f"  Future: {stats['future']}")
            print(f"  Date range: {stats['first_date']} to {stats['last_date']}")

            return True

        except Exception as e:
            print(f"‚úó Error importing overall predictions: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def import_store_predictions(self):
        """Import store-specific predictions"""
        print("\n" + "=" * 60)
        print("IMPORTING STORE PREDICTIONS")
        print("=" * 60)

        forecasts_dir = self.results_dir / 'store_forecasts_all'

        if not forecasts_dir.exists():
            print(f"‚ö†Ô∏è  Store forecasts directory not found: {forecasts_dir}")
            return False

        try:
            # Get all store forecast files
            forecast_files = list(forecasts_dir.glob('store_*_forecast.csv'))
            print(f"T√¨m th·∫•y {len(forecast_files)} store forecast files")

            if len(forecast_files) == 0:
                print("‚ö†Ô∏è  No store forecast files found")
                return False

            # Clear existing data (already done in metadata import)
            # self.db.execute_query("DELETE FROM store_predictions")

            insert_query = """
                INSERT INTO store_predictions
                (store_nbr, ds, yhat, yhat_lower, yhat_upper, is_historical)
                VALUES (%s, %s, %s, %s, %s, %s)
            """

            today = datetime.now().date()
            total_imported = 0

            for i, forecast_file in enumerate(forecast_files, 1):
                # Extract store number from filename
                try:
                    store_nbr = int(forecast_file.stem.split('_')[1])
                except (ValueError, IndexError):
                    print(f"  ‚ö†Ô∏è  Skipping invalid filename: {forecast_file.name}")
                    continue

                # Read CSV
                df = pd.read_csv(forecast_file)

                # Prepare data
                data = []
                for _, row in df.iterrows():
                    forecast_date = pd.to_datetime(row['ds']).date()
                    is_historical = forecast_date < today

                    data.append((
                        store_nbr,
                        forecast_date,
                        float(row['yhat']),
                        float(row['yhat_lower']) if 'yhat_lower' in row else None,
                        float(row['yhat_upper']) if 'yhat_upper' in row else None,
                        is_historical
                    ))

                # Insert data
                self.db.execute_many(insert_query, data)
                total_imported += len(data)

                print(f"  [{i}/{len(forecast_files)}] Store {store_nbr}: {len(data)} predictions", end='\r')

            print(f"\n‚úì Imported {total_imported} store predictions from {len(forecast_files)} stores")

            return True

        except Exception as e:
            print(f"‚úó Error importing store predictions: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def import_yearly_summary(self):
        """Import yearly forecast summary"""
        print("\n" + "=" * 60)
        print("IMPORTING YEARLY SUMMARY")
        print("=" * 60)

        summary_file = self.results_dir / 'yearly_forecast_summary.csv'

        if not summary_file.exists():
            print(f"‚ö†Ô∏è  Summary file not found: {summary_file}")
            return False

        try:
            # Read CSV
            df = pd.read_csv(summary_file)
            print(f"ƒê·ªçc {len(df)} years t·ª´ CSV")

            # Clear existing data
            self.db.execute_query("DELETE FROM yearly_forecast_summary")

            # Prepare data
            insert_query = """
                INSERT INTO yearly_forecast_summary
                (year, avg_daily, total, std, total_lower, total_upper, total_m)
                VALUES (%s, %s, %s, %s, %s, %s, %s)
            """

            data = []
            for _, row in df.iterrows():
                data.append((
                    int(row['Year']),
                    float(row['Avg_Daily']),
                    float(row['Total']),
                    float(row['Std']),
                    float(row['Total_Lower']),
                    float(row['Total_Upper']),
                    float(row['Total_M'])
                ))

            # Insert data
            self.db.execute_many(insert_query, data)
            print(f"‚úì Imported {len(data)} yearly summaries")

            return True

        except Exception as e:
            print(f"‚úó Error importing yearly summary: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def import_model_metrics(self):
        """Import model performance metrics"""
        print("\n" + "=" * 60)
        print("IMPORTING MODEL METRICS")
        print("=" * 60)

        metrics_file = self.results_dir / 'model_metrics.csv'

        if not metrics_file.exists():
            print(f"‚ö†Ô∏è  Metrics file not found: {metrics_file}")
            return False

        try:
            # Read CSV
            df = pd.read_csv(metrics_file)
            print(f"ƒê·ªçc {len(df)} metrics t·ª´ CSV")

            # Clear existing data
            self.db.execute_query("DELETE FROM model_metrics WHERE model_type = 'overall'")

            # Prepare data
            insert_query = """
                INSERT INTO model_metrics
                (model_type, store_nbr, metric_name, metric_value, unit)
                VALUES (%s, %s, %s, %s, %s)
            """

            data = []
            for _, row in df.iterrows():
                data.append((
                    'overall',
                    None,
                    str(row['Metric']),
                    float(row['Value']),
                    str(row['Unit']) if 'Unit' in row else None
                ))

            # Insert data
            self.db.execute_many(insert_query, data)
            print(f"‚úì Imported {len(data)} model metrics")

            return True

        except Exception as e:
            print(f"‚úó Error importing model metrics: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def generate_sample_insights(self):
        """Generate sample insights based on imported data"""
        print("\n" + "=" * 60)
        print("GENERATING AI INSIGHTS")
        print("=" * 60)

        try:
            # Clear old insights
            self.db.execute_query("DELETE FROM ai_insights")

            insights = []

            # 1. Top performing stores
            top_stores = self.db.fetch_all("""
                SELECT store_nbr, city, total_revenue, avg_daily_sales
                FROM store_metadata
                ORDER BY total_revenue DESC
                LIMIT 3
            """)

            if top_stores:
                top_store = top_stores[0]
                insights.append((
                    'recommendation',
                    top_store['store_nbr'],
                    f"C·ª≠a h√†ng {top_store['store_nbr']} ({top_store['city']}) d·∫´n ƒë·∫ßu v·ªÅ doanh thu",
                    f"C·ª≠a h√†ng n√†y ƒë·∫°t doanh thu cao nh·∫•t v·ªõi ${top_store['total_revenue']:,.2f}. "
                    f"Trung b√¨nh ${top_store['avg_daily_sales']:,.2f}/ng√†y. "
                    f"N√™n nghi√™n c·ª©u v√† √°p d·ª•ng m√¥ h√¨nh th√†nh c√¥ng n√†y cho c√°c c·ª≠a h√†ng kh√°c.",
                    'info',
                    json.dumps({'total_revenue': float(top_store['total_revenue']), 'rank': 1})
                ))

            # 2. Weekend revenue pattern
            weekend_data = self.db.fetch_one("""
                SELECT
                    AVG(CASE WHEN DAYOFWEEK(ds) IN (1, 7) THEN yhat ELSE NULL END) as weekend_avg,
                    AVG(CASE WHEN DAYOFWEEK(ds) NOT IN (1, 7) THEN yhat ELSE NULL END) as weekday_avg
                FROM overall_predictions
                WHERE is_historical = TRUE
            """)

            if weekend_data and weekend_data['weekend_avg'] and weekend_data['weekday_avg']:
                diff_pct = ((weekend_data['weekend_avg'] - weekend_data['weekday_avg']) /
                           weekend_data['weekday_avg'] * 100)

                if diff_pct > 10:
                    insights.append((
                        'revenue_alert',
                        None,
                        'Doanh thu cu·ªëi tu·∫ßn cao h∆°n ƒë√°ng k·ªÉ',
                        f"Doanh thu cu·ªëi tu·∫ßn cao h∆°n {diff_pct:.1f}% so v·ªõi ng√†y th∆∞·ªùng. "
                        f"ƒê·ªÅ xu·∫•t: TƒÉng nh√¢n vi√™n v√† chu·∫©n b·ªã ƒë·ªß nguy√™n li·ªáu cho cu·ªëi tu·∫ßn.",
                        'info',
                        json.dumps({'weekend_avg': float(weekend_data['weekend_avg']),
                                   'weekday_avg': float(weekend_data['weekday_avg']),
                                   'diff_pct': float(diff_pct)})
                    ))

            # 3. Next 7 days forecast
            next_week = self.db.fetch_one("""
                SELECT SUM(yhat) as total_forecast, AVG(yhat) as avg_daily
                FROM overall_predictions
                WHERE ds >= CURDATE() AND ds <= DATE_ADD(CURDATE(), INTERVAL 7 DAY)
            """)

            if next_week and next_week['total_forecast']:
                insights.append((
                    'revenue_alert',
                    None,
                    'D·ª± b√°o doanh thu 7 ng√†y t·ªõi',
                    f"T·ªïng doanh thu d·ª± ki·∫øn 7 ng√†y t·ªõi: ${next_week['total_forecast']:,.2f}. "
                    f"Trung b√¨nh ${next_week['avg_daily']:,.2f}/ng√†y.",
                    'info',
                    json.dumps({'total_forecast': float(next_week['total_forecast']),
                               'avg_daily': float(next_week['avg_daily'])})
                ))

            # 4. Stores needing improvement
            bottom_stores = self.db.fetch_all("""
                SELECT store_nbr, city, avg_daily_sales
                FROM store_metadata
                ORDER BY avg_daily_sales ASC
                LIMIT 3
            """)

            if bottom_stores and len(bottom_stores) > 0:
                bottom_store = bottom_stores[0]
                insights.append((
                    'recommendation',
                    bottom_store['store_nbr'],
                    f"C·ª≠a h√†ng {bottom_store['store_nbr']} c·∫ßn c·∫£i thi·ªán",
                    f"C·ª≠a h√†ng t·∫°i {bottom_store['city']} c√≥ doanh thu th·∫•p nh·∫•t "
                    f"(${bottom_store['avg_daily_sales']:,.2f}/ng√†y). "
                    f"ƒê·ªÅ xu·∫•t: Ki·ªÉm tra v·ªã tr√≠, marketing, v√† ch·∫•t l∆∞·ª£ng d·ªãch v·ª•.",
                    'warning',
                    json.dumps({'avg_daily_sales': float(bottom_store['avg_daily_sales'])})
                ))

            # Insert insights
            if insights:
                insert_query = """
                    INSERT INTO ai_insights
                    (insight_type, store_nbr, title, description, severity, metadata)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """

                self.db.execute_many(insert_query, insights)
                print(f"‚úì Generated {len(insights)} AI insights")
            else:
                print("‚ö†Ô∏è  No insights generated")

            return True

        except Exception as e:
            print(f"‚úó Error generating insights: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def show_summary(self):
        """Show import summary"""
        print("\n" + "=" * 60)
        print("IMPORT SUMMARY")
        print("=" * 60)

        try:
            # Count records
            stores = self.db.fetch_one("SELECT COUNT(*) as count FROM store_metadata")
            overall = self.db.fetch_one("SELECT COUNT(*) as count FROM overall_predictions")
            store_pred = self.db.fetch_one("SELECT COUNT(*) as count FROM store_predictions")
            yearly = self.db.fetch_one("SELECT COUNT(*) as count FROM yearly_forecast_summary")
            metrics = self.db.fetch_one("SELECT COUNT(*) as count FROM model_metrics")
            insights = self.db.fetch_one("SELECT COUNT(*) as count FROM ai_insights")

            print(f"‚úì Store Metadata:          {stores['count']:6d} stores")
            print(f"‚úì Overall Predictions:     {overall['count']:6d} records")
            print(f"‚úì Store Predictions:       {store_pred['count']:6d} records")
            print(f"‚úì Yearly Summaries:        {yearly['count']:6d} years")
            print(f"‚úì Model Metrics:           {metrics['count']:6d} metrics")
            print(f"‚úì AI Insights:             {insights['count']:6d} insights")

            # Date ranges
            date_range = self.db.fetch_one("""
                SELECT MIN(ds) as first_date, MAX(ds) as last_date
                FROM overall_predictions
            """)

            if date_range:
                print(f"\nüìÖ Prediction Date Range: {date_range['first_date']} ‚Üí {date_range['last_date']}")

            # Next 7 days forecast
            next_week = self.db.fetch_one("""
                SELECT
                    COUNT(*) as days,
                    SUM(yhat) as total,
                    AVG(yhat) as avg_daily
                FROM overall_predictions
                WHERE ds >= CURDATE() AND ds <= DATE_ADD(CURDATE(), INTERVAL 7 DAY)
            """)

            if next_week and next_week['days'] > 0:
                print(f"\nüí∞ Next 7 Days Forecast:")
                print(f"   Total: ${next_week['total']:,.2f}")
                print(f"   Avg:   ${next_week['avg_daily']:,.2f}/day")

        except Exception as e:
            print(f"Error showing summary: {str(e)}")

    def run_all(self):
        """Run all import tasks"""
        print("\n" + "=" * 60)
        print("ML PREDICTIONS ‚Üí DATABASE IMPORT")
        print("=" * 60)
        print(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        tasks = [
            ("Store Metadata", self.import_store_metadata),
            ("Overall Predictions", self.import_overall_predictions),
            ("Store Predictions", self.import_store_predictions),
            ("Yearly Summary", self.import_yearly_summary),
            ("Model Metrics", self.import_model_metrics),
            ("AI Insights", self.generate_sample_insights),
        ]

        results = []
        for task_name, task_func in tasks:
            success = task_func()
            results.append((task_name, success))

        # Summary
        self.show_summary()

        # Final results
        print("\n" + "=" * 60)
        print("FINAL RESULTS")
        print("=" * 60)

        success_count = sum(1 for _, success in results if success)
        total_count = len(results)

        for task_name, success in results:
            status = "‚úì" if success else "‚úó"
            print(f"{status} {task_name}")

        print(f"\n{success_count}/{total_count} tasks completed successfully")

        if success_count == total_count:
            print("\nüéâ ALL TASKS COMPLETED SUCCESSFULLY!")
            print("\nB·∫°n c√≥ th·ªÉ test AI Agent ngay b√¢y gi·ªù:")
            print("  python test_ai_agent.py")
        else:
            print("\n‚ö†Ô∏è  Some tasks failed. Check errors above.")

        print(f"\nCompleted at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("=" * 60)


def main():
    """Main function"""
    importer = PredictionImporter()
    importer.run_all()


if __name__ == "__main__":
    main()
